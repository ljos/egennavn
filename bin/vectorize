#!/usr/bin/env python3
#
# Copyright (C) 2016  Bjarte Johansen
# Copyright (C) 2016  Truls Pedersen
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
"""
vectorize

Usage:
  vectorize [ FILE ]
"""

import sys
from docopt import docopt
from funcparserlib.lexer import make_tokenizer
from funcparserlib.parser import oneplus, many, some
from keys_map import *


def binarize(sentence):
    pad = [(None, None, None, [None])] * 2
    sentence = pad + sentence + pad
    for i in range(2, len(sentence) - 2):
        codes = []

        # lemma - 2
        entry = sentence[i - 2]
        val = keys_column_1.get(entry[2], None)
        codes.append(val)


        # lemma - 1
        entry = sentence[i - 1]
        val = keys_column_2.get(entry[2], None)
        codes.append(val)

        # lemma + 1
        entry = sentence[i + 1]
        val = keys_column_3.get(entry[2], None)
        codes.append(val)

        # lemma + 2
        entry = sentence[i + 2]
        val = keys_column_4.get(entry[2], None)
        codes.append(val)

        # part of speech - 2
        entry = sentence[i - 2]
        val = keys_column_5.get(entry[3][0], None)
        codes.append(val)

        # part of speech - 1
        entry = sentence[i - 1]
        val = keys_column_6.get(entry[3][0], None)
        codes.append(val)

        # part of speech + 1
        entry = sentence[i + 1]
        val = keys_column_7.get(entry[3][0], None)
        codes.append(val)

        # part of speech + 2
        entry = sentence[i + 2]
        val = keys_column_8.get(entry[3][0], None)
        codes.append(val)

        # caps
        entry = sentence[i]
        if entry[0].isupper():
            codes.append(keys_column_9["all.cap"])
        elif entry[0][0].isupper():
            codes.append(keys_column_9["cap"])

        # last 4 characters
        last4 = entry[0][-4:].lower()
        val = keys_column_10.get(last4, None)
        codes.append(val)

        # lemma
        lemma = entry[2]
        val = keys_column_11.get(lemma, None)
        codes.append(val)

        # part of speech
        val = keys_column_12.get(entry[3][0], None)
        codes.append(val)

        print("1", " ".join([str(x)+":1" for x in codes if x is not None]))

def parse(string):
    def tokenize(string):
        specs = [
            ('Word', (r'<word>.+</word>',)),
            ('Cohort', (r'"<.+>"',)),
            ('Reading', (r'".+"',)),
            ('Space', (r'\s+',)),
            ('NL', (r'[\r\n]+',)),
            ('PoS', (r'\S+',))
        ]
        useless = ['NL', 'Space']
        t = make_tokenizer(specs)
        return [x for x in t(string) if x.type not in useless]

    tokens = tokenize(string)

    def tokval(x):
        return x.value

    def toktype(t):
        return some(lambda x: x.type == t) >> tokval

    def make_word(val):
        word, (cohort, (lemma, pos)) = val
        return (word[6:-7], cohort[2:-2], lemma[1:-1], pos)

    pos = toktype('PoS')
    reading = toktype('Reading') + many(pos)
    cohort = toktype('Cohort') + reading
    word = toktype('Word') + cohort >> make_word
    parser = many(word)

    return parser.parse(tokens)

def sentences(doc):
    sentence = []
    for word in doc:
        sentence.append(word)
        pos = word[-1]
        if '<<<' in pos:
            yield sentence
            sentence = []

def main():
    args = docopt(__doc__, version='0.1.0')

    with open(args['FILE']) if args['FILE'] else sys.stdin as f:
        text = f.read()

    document = parse(text)

    for sentence in sentences(document):
        binarize(sentence)


if __name__ == "__main__":
    main()
